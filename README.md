# DSGA-1016-finalproject
## Abstract
Deep neural networks have achieved remarkable success across various domains, including computer vision and natural language processing, through extensive training on real-world stimuli and learning representations of stimuli using thousands of features. These representations, as different sources of input, should produce similar conceptual systems, given that they are different viewpoints of the same underlying reality. 

This paper investigates the alignment of conceptual systems in image-word representations generated by deep neural networks. We analyze the image representations learned by a deep neural network trained for supervised classification tasks and compare them with the word embeddings of the corresponding labels. Our results demonstrate that different conceptual systems trained separately can align with each other and that the vision model trained on classification tasks has substantial power in revealing the higher-level structure of concepts. Overall, this study sheds light on the complex relationship between language and vision and provides insights into the mechanisms underlying conceptual representation and categorization.

## Acknowledgments
This work was part of the course work for the DS-GA 1016 Computational Cognitive Modeling course in Spring 2023. It was supported by the instructors of the course, as well as the Center for Data Science at New York University.

## Contribution
For the analysis work, Cindy worked on image category selection and image representations extractation. Bella worked on correlation analysis, including similarity matrices computation and alignment analysis. Zoe worked on clustering analysis including hierarchical clustering and t-SNE 2D visualization. 

For the writing of this paper, Cindy wrote the introduction, data pre-processing in Method part, and discussion; Bella wrote the correlation analysis in method part and the result part; Zoe wrote the clustering analysis in method part and the result part.
